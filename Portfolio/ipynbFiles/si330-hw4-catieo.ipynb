{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 330: Homework 4: APIs on AWS\n",
    "\n",
    "\n",
    "## Due: Friday, February 9, 2018,  11:59:00pm\n",
    "\n",
    "### Submission instructions</font>\n",
    "After completing this homework, you will turn in two files via Canvas ->  Assignments -> HW 4:\n",
    "Your Notebook, named si330-hw4-YOUR_UNIQUE_NAME.ipynb and\n",
    "the HTML file, named si330-hw4-YOUR_UNIQUE_NAME.html.\n",
    "\n",
    "### Name:  Catie Olson\n",
    "### Uniqname: catieo\n",
    "### People you worked with: I worked by myself. \n",
    "\n",
    "## Top-Level Goal\n",
    "To create a microservice that returns the counts of all bigrams in a text passage.\n",
    "\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "After completing this Lab, you should know how to:\n",
    "* create an AWS Lambda function that takes a string and returns the counts of all bigrams in that text\n",
    "* write an AWS API Gateway integration which allows both GET and POST requests to access an AWS Lambda\n",
    "* write documenation to the microservice that you've created\n",
    "\n",
    "### Note: See end of notebook for notes about going \"Above and Beyond\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of Steps For Analysis\n",
    "Here's an overview of the steps that you'll need to do to complete this lab.\n",
    "2. Upload data to an S3 bucket\n",
    "1. Create an AWS Lambda function that normalizes, tokenizes, and creates and counts bigrams from text, both via a POST request with the text and via a GET request to a URL that returns the text (e.g. an S3 bucket)\n",
    "3. Create a python code block in this notebook to demonstrate the functionality of your microservice\n",
    "\n",
    "Each of these steps is detailed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload data to an S3 bucket\n",
    "To get ready to test the POST functionality of the code you generate in the next step, you should upload a text file that is **500 or fewer lines** to an S3 bucket.  See the description of CORS for an explanation of why we want to put the data in the same domain (amazonaws.com) as the Lambda.\n",
    "\n",
    "Follow the same approach that we used in the lab to upload a small text file to your S3 bucket, ensuring that the permissions are set to allow public access\n",
    "\n",
    "### <font color=\"magenta\">Q1: Enter the URL of your text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://s3.amazonaws.com/si330-hw4-catieo/dickens-totc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create an AWS Lambda function that normalizes, tokenizes, and creates and counts bigrams from text\n",
    "\n",
    "Similar to what we did in the lab, you're going to create a microservice that consists of two parts: an AWS Lambda and an API Gateway.  You can use exactly the same technique that we did in the lab to get started.\n",
    "\n",
    "You will need to modify the code in the Lambda to handle two types of requests:\n",
    "1. A GET request with a queryStringParameter of url=http://some.url.goes.here/text.txt, which specifies the location of the text to be processed and\n",
    "2. A POST request with the text to be processed included as the \"text\" value in the body payload.\n",
    "\n",
    "### The following code block is a reasonable starting point for creating your Lambda.  Note that this code should not be run in this notebook but rather serve as the starting point for your work in the Lambda editor.\n",
    "\n",
    "**NOTE** Please see https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python for hints about how to create bigrams without NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PUT SOME DOCUMENTATION HERE\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from botocore.vendored import requests # This line has been added. \n",
    "# You'll need to figure out how to use this requests, \n",
    "# but it works the same way as the requests module (called using ```import requests```) in python.\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    method = event['httpMethod']\n",
    "    text = \"\"\n",
    "    d = {\"text\": \"\"}\n",
    "    # Handle GET method\n",
    "    if method == 'GET':\n",
    "        params = event['queryStringParameters']\n",
    "        if params:\n",
    "            url = ... # retrieve the text from the URL\n",
    "    if method == 'POST':\n",
    "        body = json.loads(event['body'])\n",
    "        if 'text' in body:\n",
    "            # do something \n",
    "    # normalize\n",
    "    # tokenize\n",
    "    # find bigrams\n",
    "    # NOTE: see https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python\n",
    "    #       for hints about how to create bigrams\n",
    "    # count bigrams\n",
    "    \n",
    "    # Note the strict format of the return dictionary\n",
    "    # It must contain these three elements, and the body\n",
    "    # must be a stringified JSON object (i.e. you have to call \n",
    "    # json.dumps on the JSON structure you're returning)\n",
    "    return { \n",
    "        \"statusCode\": 200,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"body\": json.dumps(d),\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2a: Enter the URL of your Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://6orkit6m27.execute-api.us-east-2.amazonaws.com/hw4v1/bigramCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2b: Copy your final Lambda code into the following code block (but do not run it here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A function that normalizes, tokenizes, and creates and counts bigrams from text\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from botocore.vendored import requests\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    method = event['httpMethod']\n",
    "    text = \"\"\n",
    "    func = \"text\"\n",
    "    d = {}\n",
    "    # Handle GET method\n",
    "    if method == 'GET':\n",
    "        params = event['queryStringParameters']\n",
    "        if params:\n",
    "            s = requests.Session()\n",
    "            url = params['url']\n",
    "            if 'func' in params:\n",
    "                func = params['func']\n",
    "            response = s.get(url)\n",
    "            text = response.text\n",
    "    if method == 'POST':\n",
    "        body = json.loads(event['body'])\n",
    "        if 'text' in body:\n",
    "            text = body['text'] \n",
    "            func = params['func']\n",
    "    if func == \"text\":\n",
    "        d[\"text\"] = text\n",
    "    \n",
    "    # normalize: convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    if func == \"normalize\":\n",
    "        d[\"normalize\"] = text\n",
    "    \n",
    "    # tokenize: split the text into sentences, the split each sentence into words\n",
    "    # NOTE: it's probably best to use re.split()\n",
    "    sentence_list = re.split('\\.\\s*|\\n\\s*', text)\n",
    "    if func == \"tokenize_sentences\":\n",
    "        d['tokenize_sentences'] = sentence_list\n",
    "    \n",
    "    word_list = re.split('\\W+', text)\n",
    "    if func == \"tokenize_words\":\n",
    "        d['tokenize_words'] = word_list\n",
    "    \n",
    "    # find bigrams\n",
    "    # NOTE: it's very difficult to set up NLTK on Lambda, so you'll need to find bigrams \"manually\"\n",
    "    # NOTE: see https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python\n",
    "    #       for hints about how to create bigrams\n",
    "    bigrams = [b for l in sentence_list for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "    if func == \"bigrams\":\n",
    "        d['bigrams'] = bigrams\n",
    "    \n",
    "    # count bigrams\n",
    "    bigram_counts = {}\n",
    "    for b in bigrams:\n",
    "        key = str(b)\n",
    "        if key not in bigram_counts:\n",
    "            bigram_counts[key] = 1\n",
    "        else:\n",
    "            bigram_counts[key] += 1\n",
    "    if func == \"bigram_counts\":\n",
    "        d['bigram_counts'] = bigram_counts\n",
    "    \n",
    "    # Note the strict format of the return dictionary\n",
    "    # It must contain these three elements, and the body\n",
    "    # must be a stringified JSON object (i.e. you have to call \n",
    "    # json.dumps on the JSON structure you're returning)\n",
    "    return { \n",
    "        \"statusCode\": 200,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"body\": json.dumps(d),\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Demonstrate the GET and POST functionality of your Lambda\n",
    "\n",
    "### <font color=\"magenta\">Q3: Create a code block that uses `requests` to demonstrate the functionality of your Lambda.  You can modify the template below or create your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bigrams': [['it', 'was'], ['was', 'the'], ['the', 'best'], ['best', 'of'], ['of', 'times,'], ['it', 'was'], ['was', 'the'], ['the', 'worst'], ['worst', 'of'], ['of', 'times,'], ['it', 'was'], ['was', 'the'], ['the', 'age'], ['age', 'of'], ['of', 'wisdom,'], ['it', 'was'], ['was', 'the'], ['the', 'age'], ['age', 'of'], ['of', 'foolishness,'], ['it', 'was'], ['was', 'the'], ['the', 'epoch'], ['epoch', 'of'], ['of', 'belief,'], ['it', 'was'], ['was', 'the'], ['the', 'epoch'], ['epoch', 'of'], ['of', 'incredulity,'], ['it', 'was'], ['was', 'the'], ['the', 'season'], ['season', 'of'], ['of', 'light,'], ['it', 'was'], ['was', 'the'], ['the', 'season'], ['season', 'of'], ['of', 'darkness,'], ['it', 'was'], ['was', 'the'], ['the', 'spring'], ['spring', 'of'], ['of', 'hope,'], ['it', 'was'], ['was', 'the'], ['the', 'winter'], ['winter', 'of'], ['of', 'despair,'], ['we', 'had'], ['had', 'everything'], ['everything', 'before'], ['before', 'us,'], ['we', 'had'], ['had', 'nothing'], ['nothing', 'before'], ['before', 'us,'], ['we', 'were'], ['were', 'all'], ['all', 'going'], ['going', 'direct'], ['direct', 'to'], ['to', 'heaven,'], ['we', 'were'], ['were', 'all'], ['all', 'going'], ['going', 'direct'], ['direct', 'the'], ['the', 'other'], ['other', 'way--'], ['in', 'short,'], ['short,', 'the'], ['the', 'period'], ['period', 'was'], ['was', 'so'], ['so', 'far'], ['far', 'like'], ['like', 'the'], ['the', 'present'], ['present', 'period,'], ['period,', 'that'], ['that', 'some'], ['some', 'of'], ['its', 'noisiest'], ['noisiest', 'authorities'], ['authorities', 'insisted'], ['insisted', 'on'], ['on', 'its'], ['its', 'being'], ['being', 'received,'], ['received,', 'for'], ['for', 'good'], ['good', 'or'], ['or', 'for'], ['evil,', 'in'], ['in', 'the'], ['the', 'superlative'], ['superlative', 'degree'], ['degree', 'of'], ['of', 'comparison'], ['comparison', 'only']]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "lambdaURL = 'https://6orkit6m27.execute-api.us-east-2.amazonaws.com/hw4v1/bigramCounter' \n",
    "textURL = 'https://s3.amazonaws.com/si330-hw4-catieo/dickens-totc.txt' \n",
    "\n",
    "# Demonstrate the GET functionality\n",
    "funcParam = \"bigrams\"\n",
    "response = requests.get(lambdaURL + '?url=' + textURL + '&func=' + funcParam)\n",
    "bigrams = json.loads(response.text)\n",
    "print(bigrams) # you should make this print something nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bigram_counts': {\"('it', 'was')\": 10, \"('was', 'the')\": 10, \"('the', 'best')\": 1, \"('best', 'of')\": 1, \"('of', 'times,')\": 2, \"('the', 'worst')\": 1, \"('worst', 'of')\": 1, \"('the', 'age')\": 2, \"('age', 'of')\": 2, \"('of', 'wisdom,')\": 1, \"('of', 'foolishness,')\": 1, \"('the', 'epoch')\": 2, \"('epoch', 'of')\": 2, \"('of', 'belief,')\": 1, \"('of', 'incredulity,')\": 1, \"('the', 'season')\": 2, \"('season', 'of')\": 2, \"('of', 'light,')\": 1, \"('of', 'darkness,')\": 1, \"('the', 'spring')\": 1, \"('spring', 'of')\": 1, \"('of', 'hope,')\": 1, \"('the', 'winter')\": 1, \"('winter', 'of')\": 1, \"('of', 'despair,')\": 1, \"('we', 'had')\": 2, \"('had', 'everything')\": 1, \"('everything', 'before')\": 1, \"('before', 'us,')\": 2, \"('had', 'nothing')\": 1, \"('nothing', 'before')\": 1, \"('we', 'were')\": 2, \"('were', 'all')\": 2, \"('all', 'going')\": 2, \"('going', 'direct')\": 2, \"('direct', 'to')\": 1, \"('to', 'heaven,')\": 1, \"('direct', 'the')\": 1, \"('the', 'other')\": 1, \"('other', 'way--')\": 1, \"('in', 'short,')\": 1, \"('short,', 'the')\": 1, \"('the', 'period')\": 1, \"('period', 'was')\": 1, \"('was', 'so')\": 1, \"('so', 'far')\": 1, \"('far', 'like')\": 1, \"('like', 'the')\": 1, \"('the', 'present')\": 1, \"('present', 'period,')\": 1, \"('period,', 'that')\": 1, \"('that', 'some')\": 1, \"('some', 'of')\": 1, \"('its', 'noisiest')\": 1, \"('noisiest', 'authorities')\": 1, \"('authorities', 'insisted')\": 1, \"('insisted', 'on')\": 1, \"('on', 'its')\": 1, \"('its', 'being')\": 1, \"('being', 'received,')\": 1, \"('received,', 'for')\": 1, \"('for', 'good')\": 1, \"('good', 'or')\": 1, \"('or', 'for')\": 1, \"('evil,', 'in')\": 1, \"('in', 'the')\": 1, \"('the', 'superlative')\": 1, \"('superlative', 'degree')\": 1, \"('degree', 'of')\": 1, \"('of', 'comparison')\": 1, \"('comparison', 'only')\": 1}}\n"
     ]
    }
   ],
   "source": [
    "s3text = requests.get(textURL) # get the text from the bucket\n",
    "d = {\"text\": s3text.text, \"func\" : \"bigram_counts\"}\n",
    "response = requests.post(lambdaURL, json = d)\n",
    "bigrams = json.loads(response.text)\n",
    "print(bigrams) # you should make this print something nicer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your notebook, download it as HTML and submit both the .ipynb and .html files to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about going \"Above and Beyond\"\n",
    "\n",
    "There are ample opportunities for extending this homework assignment.  You might, for example, decide to break the microservice into three separate ones (normalizing, tokenizing, and creating bigrams).  Alternatively, you might invest time into getting NLTK data into Lambda so you can use its functionality (see https://stackoverflow.com/questions/42394335/paths-in-aws-lambda-with-python-nltk).  Another interesting investigation might be to use the addition of a data file to an S3 bucket as a trigger to run the bigram analysis, perhaps writing the results to another (public) bucket.\n",
    "\n",
    "**IF YOU CHOOSE TO GO ABOVE AND BEYOND, YOU _MUST_ CHANGE THE FOLLOWING MARKDOWN BLOCK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above and Beyond\n",
    "\n",
    "Indicate here why you believe that your work should be considered \"above and beyond\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "My lambda function accepts a second parameter called \"func\" that accepts the following values: \"text\", \"normalize\", \"tokenize_sentences\", \"tokenize_words\", \"bigrams\", or \"bigram_counts\" and returns the text manipulated according to what was specified in the func parameter. The func param would be passed with the requests.get or requests.post and would be in event['queryStringParameters']. Two examples are shown above with bigrams and bigram_counts.  If no func parameter is specified, the text is returned un-manipulated. The other specified func parameters are demonstrated below with GET requests, but will also work with POST requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'It was the best of times,\\nit was the worst of times,\\nit was the age of wisdom,\\nit was the age of foolishness,\\nit was the epoch of belief,\\nit was the epoch of incredulity,\\nit was the season of Light,\\nit was the season of Darkness,\\nit was the spring of hope,\\nit was the winter of despair,\\nwe had everything before us,\\nwe had nothing before us,\\nwe were all going direct to Heaven,\\nwe were all going direct the other way--\\nin short, the period was so far like the present period, that some of\\nits noisiest authorities insisted on its being received, for good or for\\nevil, in the superlative degree of comparison only.'}\n"
     ]
    }
   ],
   "source": [
    "funcParam = \"text\"\n",
    "response = requests.get(lambdaURL + '?url=' + textURL + '&func=' + funcParam)\n",
    "bigrams = json.loads(response.text)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'normalize': 'it was the best of times,\\nit was the worst of times,\\nit was the age of wisdom,\\nit was the age of foolishness,\\nit was the epoch of belief,\\nit was the epoch of incredulity,\\nit was the season of light,\\nit was the season of darkness,\\nit was the spring of hope,\\nit was the winter of despair,\\nwe had everything before us,\\nwe had nothing before us,\\nwe were all going direct to heaven,\\nwe were all going direct the other way--\\nin short, the period was so far like the present period, that some of\\nits noisiest authorities insisted on its being received, for good or for\\nevil, in the superlative degree of comparison only.'}\n"
     ]
    }
   ],
   "source": [
    "funcParam = \"normalize\"\n",
    "response = requests.get(lambdaURL + '?url=' + textURL + '&func=' + funcParam)\n",
    "bigrams = json.loads(response.text)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenize_sentences': ['it was the best of times,', 'it was the worst of times,', 'it was the age of wisdom,', 'it was the age of foolishness,', 'it was the epoch of belief,', 'it was the epoch of incredulity,', 'it was the season of light,', 'it was the season of darkness,', 'it was the spring of hope,', 'it was the winter of despair,', 'we had everything before us,', 'we had nothing before us,', 'we were all going direct to heaven,', 'we were all going direct the other way--', 'in short, the period was so far like the present period, that some of', 'its noisiest authorities insisted on its being received, for good or for', 'evil, in the superlative degree of comparison only', '']}\n"
     ]
    }
   ],
   "source": [
    "funcParam = \"tokenize_sentences\"\n",
    "response = requests.get(lambdaURL + '?url=' + textURL + '&func=' + funcParam)\n",
    "bigrams = json.loads(response.text)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenize_words': ['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times', 'it', 'was', 'the', 'age', 'of', 'wisdom', 'it', 'was', 'the', 'age', 'of', 'foolishness', 'it', 'was', 'the', 'epoch', 'of', 'belief', 'it', 'was', 'the', 'epoch', 'of', 'incredulity', 'it', 'was', 'the', 'season', 'of', 'light', 'it', 'was', 'the', 'season', 'of', 'darkness', 'it', 'was', 'the', 'spring', 'of', 'hope', 'it', 'was', 'the', 'winter', 'of', 'despair', 'we', 'had', 'everything', 'before', 'us', 'we', 'had', 'nothing', 'before', 'us', 'we', 'were', 'all', 'going', 'direct', 'to', 'heaven', 'we', 'were', 'all', 'going', 'direct', 'the', 'other', 'way', 'in', 'short', 'the', 'period', 'was', 'so', 'far', 'like', 'the', 'present', 'period', 'that', 'some', 'of', 'its', 'noisiest', 'authorities', 'insisted', 'on', 'its', 'being', 'received', 'for', 'good', 'or', 'for', 'evil', 'in', 'the', 'superlative', 'degree', 'of', 'comparison', 'only', '']}\n"
     ]
    }
   ],
   "source": [
    "funcParam = \"tokenize_words\"\n",
    "response = requests.get(lambdaURL + '?url=' + textURL + '&func=' + funcParam)\n",
    "bigrams = json.loads(response.text)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
